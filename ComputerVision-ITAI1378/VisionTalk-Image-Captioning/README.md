# VisionTalk: Image Captioning Program

## 1. Project Overview
VisionTalk is an image captioning system that generates natural-language descriptions for images by combining computer vision with language modeling. The goal of this project is to demonstrate how modern AI models interpret visual information and translate it into meaningful text.

This work reflects concepts learned throughout the ITAI 1378 Computer Vision course and applies them in a real, functional system.

---

## 2. Problem Statement
Humans describe images effortlessly, but teaching a computer to do the same is a complex challenge.  
Image captioning requires an AI model to:

1. Understand what is shown in the image  
2. Extract meaningful visual features  
3. Convert those features into coherent language  

This project examines how a Vision-Language Model (VLM) bridges these two domains by combining image processing and natural-language generation.

---

## 3. Approach
I implemented an image captioning pipeline using the **BLIP Vision-Language Model** from Hugging Face. The notebook:

- Loads a pre-trained VLM  
- Processes the uploaded image  
- Extracts visual features  
- Generates a natural-language caption  

The system demonstrates how deep learning techniques can be integrated to interpret images and produce descriptive text with minimal manual intervention.

---

## 4. Methods and Technologies Used

### **Core Concepts**
- Image processing  
- Feature extraction  
- Neural networks  
- Convolutional Neural Networks (CNNs)  
- Transformers  
- Vision-Language modeling  

### **Tools & Libraries**
- Python  
- Hugging Face Transformers  
- BLIP (Bootstrapped Language-Image Pretraining)  
- PyTorch  
- Jupyter Notebook / Google Colab  
- NumPy, Matplotlib, PIL  

---

## 5. Dataset Information
This project does not require a custom dataset.  
Images are provided by the user at runtime, and the model processes them directly.

No public datasets (e.g., COCO, Pascal VOC) are uploaded to GitHub in order to follow data-handling guidelines.

---

## 6. Results
The model successfully generates captions that describe the content of user-uploaded images. The results demonstrate:

- Accurate recognition of objects  
- Ability to describe scenes in natural language  
- Strong alignment between visual and linguistic representations  

Sample output examples can be found in the **results** folder.

---

## 7. How to Run the Notebook
1. Open the Jupyter Notebook (`VisionTalk_Image_Captioning.ipynb`)  
2. Install required libraries (Hugging Face Transformers, PyTorch)  
3. Upload an image when prompted  
4. Run all cells to generate a caption  

The notebook is compatible with both local environments and Google Colab.

---

## 8. Key Takeaways
From this project, I gained experience in:

- Working with pre-trained AI models  
- Understanding how computer vision and NLP merge inside VLMs  
- Formatting real-world AI inference pipelines  
- Improving model outputs through testing and evaluation  
- Documenting and organizing project work for a professional portfolio  

---

## 9. Files in This Folder
VisionTalk-Image-Captioning/
│
├── README.md # Project overview and documentation
├── VisionTalk_Image_Captioning.ipynb # Main image captioning notebook
└── results/ # Sample outputs and generated captions
└── (example images + results)


---

## 10. Future Improvements
- Add support for multiple captions per image  
- Compare BLIP with other models such as ViT-GPT2 or CLIP-based captioners  
- Deploy as a simple web app interface  
- Integrate speech output for accessibility  

---

## 11. Contact
**Gregory Livingston**  
Applied Artificial Intelligence & Robotics Student  
Houston Community College  

Email: w216359933@student.hccs.edu  
LinkedIn: https://www.linkedin.com/in/greglivin/  
GitHub Portfolio: *(link to your main repo)*  

---


