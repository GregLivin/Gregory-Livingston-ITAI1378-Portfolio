ðŸŽ¯ VisionTalk â€“ Image Captioning System

VisionTalk is an AI-powered image captioning system built using a BLIP Visionâ€“Language Model.
This project demonstrates how artificial intelligence can connect visual understanding with natural-language generation, producing captions that describe the content of an input image.
-
1. Problem Statement

Image captioning combines computer vision and natural language processing.
The goal is to create a system that:

Accepts an image

Understands its visual features

Generates a meaningful caption in natural language

This project applies concepts learned throughout the course, including image preprocessing, neural networks, transfer learning, and multimodal AI.
-
2. Approach

The captioning system is powered by a pretrained BLIP Visionâ€“Language Model provided by Hugging Face.

The notebook performs the following steps:

Load a pretrained BLIP processor and model

Accept an image input (uploaded or linked)

Prepare and transform the image

Run inference through the model

Generate a descriptive caption

Display the original image and its caption

This project integrates everything learned in ITAI 1378, from image processing fundamentals to transformer-based AI.
-
3. Files in This Folder
VisionTalk-Image-Captioning/
â”œâ”€â”€ VisionTalk_Image_Captioning.ipynb   # Main notebook
â”œâ”€â”€ README.md                           # Project documentation
â””â”€â”€ results/                            # Sample outputs


VisionTalk_Image_Captioning.ipynb
The full notebook containing code, explanations, and example outputs.

results/
Screenshots of model outputs, such as images paired with generated captions.
-
4. How to Run the Notebook
Option A â€“ Run in Google Colab (Recommended)

Open Google Colab

Upload the notebook or open it directly from GitHub

Install dependencies:

!pip install transformers timm pillow accelerate


Upload an image or use one included in the notebook

Run all cells in order

Option B â€“ Run Locally

Install Python 3.8 or higher

Install dependencies:

pip install transformers timm pillow accelerate


Launch Jupyter Notebook:

jupyter notebook


Open the notebook and run all cells
-
5. Dataset and Images

This project does not require a large dataset.
It uses standalone images for testing caption generation.

If images are added:

Keep them small

Place them in the notebook or results/ folder

Provide citation if sourced externally
-
6. Results & Examples

Screenshots and output examples are saved in the results/ folder.

These show:

Input images

Captions generated by the model

Any additional visualizations produced during testing
-
7. What I Learned

Through this project I gained experience with:

Working with Visionâ€“Language Models (VLMs)

Running pretrained models with Hugging Face

Connecting images to text generation

Structuring a complete AI mini-project

Documenting outputs for a professional portfolio

This project reflects my progress in the Applied AI & Robotics Program and serves as a foundation for future multimodal and robotics applications.
